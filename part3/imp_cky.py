from tree import *
import os
import numpy as np
import time
from old_cky import CkyViterbi
import sys
class ImpCky(object):
    def __init__(self,train_file, dev_file,test_file):
        self.grammar = {}
        self.sorted_rules = []
        self.non_terms= []
        self.term_lookup = {}
        self.back = []
        self.vocabulary = []
        self.dev_parses = []
        self.test_parses = []
        self.train_file = train_file
        self.dev_file = dev_file
        self.test_file = test_file
    def compose(self,model_1, model_2):
        self.model_1 = model_1
        self.model_2 = model_2
    def sim_models(self):
        self.model_1.parse_dev('./part3/output/dev.parses.horiz')
        self.model_2.parse_dev('./part3/output/dev.parses.old')
        self.model_1.parse_test('./part3/output/test.parses.horiz')
        self.model_2.parse_test('./part3/output/test.parses.old')
    def main(self):
        
        self.read_training()  
        self.add_delta_smooth(delta=0.6)  
        unique_rules = self.convert_probs()
        self.prep_syms()

    def prep_syms(self):
        self.non_terms = set(self.non_terms)
        self.non_terms = sorted(self.non_terms)
        for i, term in enumerate(self.non_terms):
           self.term_lookup[term ] = i 
        self.vocabulary = set(self.vocabulary)
    def parse_dev(self,out_name):
        sentence_lengths = []
        times = []
        
        dev_parse = open(out_name, 'w')
        
        for i,line in enumerate(self.dev_file):
            line = line.rstrip()
            line = line.split()
       
            for j,word in enumerate(line):
                if word not in self.vocabulary:
                    line[j] = '<unk>' 

            parse= self.cky_parse( line , i )
            #print('\n' + parse + '\ni=' + str(i))
            self.dev_parses.append(parse)
            dev_parse.write(parse)
            dev_parse.write('\n')

    def parse_test(self,out_name):
        sentence_lengths = []
        dev_parse = open(out_name, 'w')
        
        for i,line in enumerate(self.test_file):
            line = line.rstrip()
            line = line.split()
       
            for j,word in enumerate(line):
                if word not in self.vocabulary:
                    line[j] = '<unk>' 

            parse= self.cky_parse( line , i,)
            self.test_parses.append(parse)


            dev_parse.write(parse)
            dev_parse.write('\n')
    def sim_all_test(self):
        test_parse = open('./part3/output/test.parses.vert', 'w')
        
        for i,line in enumerate(self.test_file):
            line = line.rstrip()
            line = line.split()
       
            for j,word in enumerate(line):
                if word not in self.vocabulary:
                    line[j] = '<unk>' 

            parse= self.cky_parse( line , i,final='yes',version='test')
            #print('\n' + parse + '\ni=' + str(i))

            test_parse.write(parse)
            test_parse.write('\n')
    def sim_all_dev(self):
 
        dev_parse = open('./part3/output/dev.parses.vert', 'w')
        
        for i,line in enumerate(self.dev_file):
            line = line.rstrip()
            line = line.split()
       
            for j,word in enumerate(line):
                if word not in self.vocabulary:
                    line[j] = '<unk>' 

            parse= self.cky_parse( line , i ,final='yes',version='dev')
            #print('\n' + parse + '\ni=' + str(i))

            dev_parse.write(parse)
            dev_parse.write('\n')

    def cky_parse(self,line, parse_index, final='no',version='dev'):

        # build matrix n by n by X
        best = []
        self.back = []

        sen_length = len(line)
        neg_inf = -1 * np.inf
        if len(line) == 0:
            return "",""
        # fill best and self.back matricies
        for i in range(0,sen_length):
            best.append([0]*(sen_length+1))  
            self.back.append([0] *( sen_length+1)) 
        for i in range(0,sen_length):
            for j in range(0,sen_length+1):
                best[i][j] = [neg_inf] * len(self.non_terms)
                self.back[i][j] = [0] * len(self.non_terms)

        # populate first diag
        # for each word in sentence
        for i in range(1, sen_length + 1):
            for gen_rule, child_rule_dict in self.grammar.items():
                # if word can be generated by this rule
                if line[i - 1] in child_rule_dict.keys():
                    # prob of this rule

                    if child_rule_dict[line[i-1]] > best[i-1][i][self.term_lookup[gen_rule]]:
              
                        best[i-1][i][self.term_lookup[gen_rule]] = child_rule_dict[line[i-1]]
                        self.back[i-1][i][self.term_lookup[gen_rule]] = [] 
                        self.back[i-1][i][self.term_lookup[gen_rule]] = [gen_rule, line[i-1],i-1,i]
        for l in range(2, sen_length+1):
            for i in range(0 , sen_length - l+1):
                j = i + l
                for k in range(i+1, j ):
                    
                    # iterate through every rule we know
                    for gen_rule, child_rules in self.grammar.items():
                        for child_rule, prob in child_rules.items():
                            # X -> Y Z  - YZ stored as string "Y Z", need to split
                            t_child_rule = child_rule.split()
                            if len(t_child_rule) == 2:
                               
                                prob_p = prob + best[i][k][self.term_lookup[t_child_rule[0]]] + best[k][j][self.term_lookup[t_child_rule[1]]]
                               
                                if prob_p > best[i][j][self.term_lookup[gen_rule]]:
                                    best[i][j][self.term_lookup[gen_rule]] = prob_p
                                    self.back[i][j][self.term_lookup[gen_rule]] = [gen_rule,t_child_rule[0],t_child_rule[1],i,k,j]

        end_rule =self.back[0][sen_length][self.term_lookup['TOP']]
        # failed parse
        if end_rule == 0 and final =='yes':
            if version=='dev':
                if self.model_1.dev_parses[parse_index] != "":
                    return self.model_1.dev_parses[parse_index].rstrip()
                else:
                    return self.model_2.dev_parses[parse_index].rstrip()
            else:
                if self.model_1.test_parses[parse_index] != "":
                    return self.model_1.test_parses[parse_index].rstrip()
                else:
                    return self.model_2.test_parses[parse_index].rstrip()

        elif end_rule == 0 and final == 'no':
            return ""
        else:
            parse = self.print_tree(end_rule, 0,sen_length )
            return parse

    def print_tree(self, X, i ,j):

        ret_string = ""
        ret_string += "("
        ret_string += X[0] + " "

        if len(X) == 4:
            ret_string = "(" + X[0] + " "   + X[1] + ")"
            return ret_string

        else:
            y_index = self.term_lookup[X[1]] 
            k = X[4]
     
            ret_string += self.print_tree(self.back[i][k][y_index],i,X[4])
            ret_string += " "
           # print( " ", end="")
            z_index = self.term_lookup[X[2]] 
            ret_string += self.print_tree(self.back[k][j][z_index],X[4],j)
           # print(")", end="")
            ret_string += ")"
            return ret_string
    def add_delta_smooth(self,delta):
       
        for parent_rule, child_rules in self.grammar.items():
            self.grammar[parent_rule]['<unk>'] = -1*np.inf
            for child_rule,count in child_rules.items():
                rule = child_rule.split()
                if len(rule) == 1:
                    self.grammar[parent_rule][child_rule] += delta
                    self.grammar[parent_rule]['<unk>'] = delta
    def convert_probs(self):

        unique = 0
        for parent_rule, child_rules in self.grammar.items():
            self.non_terms.append(parent_rule)
            unique += len(child_rules.keys())       
            child_sum = sum(child_rules.values())
            for child_rule, count in child_rules.items():
                full_rule = parent_rule + ' -> ' + child_rule
                prob = count / child_sum
                prob = np.log(prob) 
                self.grammar[parent_rule][child_rule] = prob
                self.sorted_rules.append((full_rule, count, prob))
                for term in child_rule.split():
                    self.non_terms.append(term)
        return unique

    def read_training(self):
        
        for line in self.train_file:
            t = Tree.from_str(line)
            self.traverse_tree(t.root)
    def traverse_tree(self,cur_node):
        cur_rule = cur_node.label
        if len(cur_node.children) == 2:
            child_rule = cur_node.children[0].label + " " +  cur_node.children[1].label  

        elif len(cur_node.children) == 1:
            child_rule = cur_node.children[0].label
            self.vocabulary.append(cur_node.children[0].label)
        else:
            return "ERR : too many children"
            
        if cur_rule in self.grammar.keys():
            if child_rule in self.grammar[cur_rule].keys():
                self.grammar[cur_rule][child_rule] += 1
            else:
                self.grammar[cur_rule][child_rule] = 1
        else:
            self.grammar[cur_rule] = {}
            self.grammar[cur_rule][child_rule] = 1.
        for node in cur_node.children:
            self.traverse_tree(node)









if __name__ == "__main__":
    imp_cky = ImpCky()
    imp_cky.main()
