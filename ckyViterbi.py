from tree import *
import os
import numpy as np
grammar = {}
sorted_rules = []
non_terms= []
term_lookup = {}
back = []
def main():
    
    read_training()    
    unique_rules = convert_probs()
    grammar_stats(unique_rules)
    prep_syms()
    parse_dev()
def prep_syms():
    global non_terms
    global term_lookup
    non_terms = set(non_terms)
    non_terms = sorted(non_terms)
    for i, term in enumerate(non_terms):
       term_lookup[term ] = i 
def parse_dev():
    dev_in = open(os.getcwd() + "/dev.strings")
    dev_lines = dev_in.readlines()
    dev_parse = open('dev.parse', 'w')
    for i,line in enumerate(dev_lines):
        dev_lines[i] = dev_lines[i].rstrip()
        cky_parse( dev_lines[i], dev_parse )

   
def cky_parse(line, out_file):
    global non_terms
    global grammar
    global term_lookup
    # build matrix n by n by X
    best = []

    line = line.split()
    sen_length = len(line)
   
    for i in range(0,sen_length):
        best.append([0]*sen_length)  
        back.append([0] * sen_length) 
        for j,val in enumerate(best[i]):
            best[i][j] = [0] * len(non_terms)
            back[i][j] = [0] * len(non_terms)
    # populate first diag
    # for each word in sentence
    for i in range(1, sen_length):
        for gen_rule, child_rule_dict in grammar.items():
            # if word can be generated by this rule
            if line[i] in child_rule_dict.keys():
                # prob of this rule
                if child_rule_dict[line[i]] > best[i-1][i][term_lookup[gen_rule]]:
                    best[i-1][i][term_lookup[gen_rule]] = child_rule_dict[line[i]]
                    back[i-1][i][term_lookup[gen_rule]] = [] 
                    back[i-1][i][term_lookup[gen_rule]] = [gen_rule, line[i],i-1,i]
    for l in range(2, sen_length):
        for i in range(0 , sen_length - l):
            j = i + l
            for k in range(i+1, j - 1):
                for gen_rule, child_rules in grammar.items():
                    for child_rule, prob in child_rules.items():
                        t_child_rule = child_rule.split()
                        if len(t_child_rule) == 2:
                           
                            prob_p = prob + best[i][k][term_lookup[t_child_rule[0]]] + best[k][j][term_lookup[t_child_rule[1]]]
                           
                            if prob_p > best[i][j][term_lookup[gen_rule]]:
                                best[i][j][term_lookup[gen_rule]] = prob_p
                                back[i][j][term_lookup[gen_rule]] = [gen_rule,t_child_rule[0],t_child_rule[1],i,k,j]
    
    #print(back[0][sen_length-1][best[0][sen_length-1].index(max( best[0][sen_length-1] ) )])
    print_tree(back[0][sen_length-1][best[0][sen_length-1].index( max( best[0][sen_length-1] ) )], 0,sen_length -1 )
    
    #print( str(back[0][sen_length-1]))

def print_tree( X, i ,j):
    global back
    global term_lookup
    print("(", end="")
    print(str(X) + ' ' + str(i) + " " + str(j))
    #rule = back[i][j][int(X)]
    if type(X) == list and len(X) != 6:
        print( X[1], end="" )

    else:
        y_index = term_lookup[X[1]] 
        print_tree(back[i][X[4]-1][y_index],i,X[4]-1)
        print( " ", end="")
        z_index = term_lookup[X[2]] 
        print_tree(back[X[4]-1][X[5]-1][z_index],X[4]-1,j-1)


def grammar_stats(unique):
    global sorted_rules
    sorted_rules = sorted( sorted_rules, key= lambda x: x[1])
    print("There are " + str(unique) + " unique rules in the grammar" )
    print("The top five rules are: ")
    sorted_rules.reverse()
    top_five = sorted_rules[0:5]
    for rule in top_five:
        print( rule[0] + " # " + str(rule[1] ))
    sorted_rules = sorted( sorted_rules, key = lambda x: x[2])
    sorted_rules.reverse()
    top_five = sorted_rules[0:5]
    print("Top five probable rules are: " )
    for rule in top_five:
        print( rule[0] + " # " + str(rule[2]))
def convert_probs():
    unique = 0
    for parent_rule, child_rules in grammar.items():
        non_terms.append(parent_rule)
        unique += len(child_rules.keys())
        for child_rule, count in child_rules.items():
            full_rule = parent_rule + ' -> ' + child_rule
            prob = count / sum(child_rules.values())
            prob = np.log(prob)
            grammar[parent_rule][child_rule] = prob
            sorted_rules.append((full_rule, count, prob))
            for term in child_rule.split():
                non_terms.append(term)
    return unique

def read_training():
    train_file = open(os.getcwd() + '/train.trees.pre.unk') 
    for line in train_file:
        t = Tree.from_str(line)
        traverse_tree(t.root)
def traverse_tree(cur_node):
    global grammar
    cur_rule = cur_node.label
    if len(cur_node.children) == 2:
        child_rule = cur_node.children[0].label + " " +  cur_node.children[1].label  

    elif len(cur_node.children) == 1:
        child_rule = cur_node.children[0].label

    else:
        return "ERR : too many children"
        
    if cur_rule in grammar.keys():
        if child_rule in grammar[cur_rule].keys():
            grammar[cur_rule][child_rule] += 1
        else:
            grammar[cur_rule][child_rule] = 1
    else:
        grammar[cur_rule] = {}
        grammar[cur_rule][child_rule] = 1.
    for node in cur_node.children:
        traverse_tree(node)









if __name__ == "__main__":
    main()
